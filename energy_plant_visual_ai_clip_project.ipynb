{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaledaldhaheri91-maker/khaled1/blob/main/energy_plant_visual_ai_clip_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2bceae5",
      "metadata": {
        "id": "a2bceae5"
      },
      "source": [
        "# Energy Plant Visual AI Demo â€“ CLIP on Hugging Face\n",
        "\n",
        "In this project, we extend the **energy plant** use case into **Visual AI** using an openâ€‘source model from Hugging Face.\n",
        "\n",
        "We will:\n",
        "- Use the **OpenAI CLIP** visionâ€“language model (via Hugging Face: `openai/clip-vit-base-patch32`)  \n",
        "- Take a **photo of an energy plant component** (e.g., turbine, valve, control panel, switchgear)  \n",
        "- Ask the model to **classify what the picture is about** using zeroâ€‘shot learning  \n",
        "- Build a small **labelling pipeline** that can be used for:\n",
        "  - Quickly tagging plant images (component type, area, risk category)  \n",
        "  - Preparing data for supervised training later  \n",
        "  - Demonstrating Visual AI in an MLOps / monitoring context\n",
        "\n",
        "> ðŸ’¡ **Workshop goal:**  \n",
        "> Show students how they can go from **raw images** â†’ **preâ€‘trained openâ€‘source model** â†’ **meaningful labels** without training a deep network from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8175d725",
      "metadata": {
        "id": "8175d725",
        "outputId": "a27ae4db-4765-4e6e-9fa7-b1cb00736831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\n",
            "Reason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 1. Install dependencies (run this once in Colab or your environment)\n",
        "# If you're running locally and already have these installed, you can skip this cell.\n",
        "\n",
        "!pip install -q transformers==4.46.0 torch pillow matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce7239af",
      "metadata": {
        "id": "ce7239af",
        "outputId": "c71b3f86-f933-4a19-9985-fe458e4c2492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model loaded: openai/clip-vit-base-patch32\n"
          ]
        }
      ],
      "source": [
        "# 2. Imports and configuration\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Use GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load CLIP model & processor from Hugging Face\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "print(\"Model loaded:\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a1535c",
      "metadata": {
        "id": "b2a1535c"
      },
      "source": [
        "## 3. Understanding CLIP in Simple Terms\n",
        "\n",
        "**CLIP** (Contrastive Languageâ€“Image Preâ€‘training) is a model that:\n",
        "\n",
        "- Takes an **image** and a set of **text prompts** (e.g., *\"a photo of a turbine\"*, *\"a photo of a control panel\"*).  \n",
        "- Embeds both into a shared space and computes **similarity scores**.  \n",
        "- The text whose embedding is closest to the image is considered the **best label**.\n",
        "\n",
        "We will use CLIP in a **zeroâ€‘shot** way:\n",
        "\n",
        "- No extra training on our side.  \n",
        "- We only define **good, descriptive prompts** for the different plant components / conditions we care about.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b15d62",
      "metadata": {
        "id": "61b15d62",
        "outputId": "3ca5a033-9957-4ad1-f94a-bc8853a7b19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a photo of a turbine in an industrial energy plant\n",
            "a photo of a control room in an industrial energy plant\n",
            "a photo of a control panel in an industrial energy plant\n",
            "a photo of a switchgear in an industrial energy plant\n",
            "a photo of a valve in an industrial energy plant\n",
            "a photo of a pipeline in an industrial energy plant\n",
            "a photo of a cooling tower in an industrial energy plant\n",
            "a photo of a boiler area in an industrial energy plant\n",
            "a photo of a pump in an industrial energy plant\n",
            "a photo of a electrical cabinet in an industrial energy plant\n"
          ]
        }
      ],
      "source": [
        "# 4. Define candidate labels for the energy plant\n",
        "\n",
        "# Plain labels (for logging / readability)\n",
        "plain_labels = [\n",
        "    \"turbine\",\n",
        "    \"control room\",\n",
        "    \"control panel\",\n",
        "    \"switchgear\",\n",
        "    \"valve\",\n",
        "    \"pipeline\",\n",
        "    \"cooling tower\",\n",
        "    \"boiler area\",\n",
        "    \"pump\",\n",
        "    \"electrical cabinet\",\n",
        "]\n",
        "\n",
        "# Text prompts for CLIP â€“ more descriptive usually works better\n",
        "text_prompts = [f\"a photo of a {lbl} in an industrial energy plant\" for lbl in plain_labels]\n",
        "\n",
        "for p in text_prompts:\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9ae274",
      "metadata": {
        "id": "2a9ae274"
      },
      "outputs": [],
      "source": [
        "# 5. Helper functions: load image, classify, and visualize\n",
        "\n",
        "def load_image(image_path: str) -> Image.Image:\n",
        "    \"\"\"Load an image from a local path.\"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    return img\n",
        "\n",
        "\n",
        "def classify_image_with_clip(\n",
        "    image: Image.Image,\n",
        "    text_prompts: List[str],\n",
        "    model: CLIPModel,\n",
        "    processor: CLIPProcessor,\n",
        "    device: str = \"cpu\",\n",
        "    top_k: int = 5,\n",
        "):\n",
        "    \"\"\"Run CLIP on a single image + list of text prompts and return top-k scores.\"\"\"\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(\n",
        "        text=text_prompts,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # CLIP returns logits_per_image: [batch_size, num_text_prompts]\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy().flatten()\n",
        "\n",
        "    # Sort prompts by probability\n",
        "    indices = probs.argsort()[::-1]  # descending\n",
        "    top_indices = indices[:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            \"prompt\": text_prompts[idx],\n",
        "            \"probability\": float(probs[idx]),\n",
        "            \"plain_label\": plain_labels[idx],\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "def show_image_with_predictions(image: Image.Image, predictions):\n",
        "    \"\"\"Display image with top-1 prediction in the title and print top-k table.\"\"\"\n",
        "    top1 = predictions[0]\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Top-1: {top1['plain_label']} ({top1['probability']:.2%})\")\n",
        "    plt.show()\n",
        "\n",
        "    # Display as a small table\n",
        "    df = pd.DataFrame(predictions)\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82371169",
      "metadata": {
        "id": "82371169"
      },
      "source": [
        "## 6. Providing Images for the Demo\n",
        "\n",
        "You have a few options for getting energy plant images into this notebook:\n",
        "\n",
        "1. **Upload manually in Colab**  \n",
        "   - Go to: `Files` panel â†’ Upload your `turbine.jpg`, `valve.jpg`, etc.  \n",
        "   - Or use a small upload cell (shown next).  \n",
        "\n",
        "2. **Use a local folder** (if running in VS Code / Jupyter locally)  \n",
        "   - Put images under a folder like `data/plant_images/`.  \n",
        "   - Refer to them by their relative path.  \n",
        "\n",
        "For the workshop, you can prepare a **small image pack** (5â€“10 photos) and share it with students.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b68130",
      "metadata": {
        "id": "a0b68130"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Colab upload helper\n",
        "# Uncomment and run this cell if you're using Google Colab and want to upload files manually.\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # This lets you pick images from your computer\n",
        "# list(uploaded.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfdf2548",
      "metadata": {
        "id": "bfdf2548",
        "outputId": "d3c3f3cf-01f1-4cec-8868-f564a4465589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image not found: example_plant_photo.jpg\n",
            "Please update `example_image_path` to point to a real file.\n"
          ]
        }
      ],
      "source": [
        "# 7. Single-image classification demo\n",
        "\n",
        "# Replace this path with one of your uploaded images, e.g. \"turbine_1.jpg\"\n",
        "example_image_path = \"example_plant_photo.jpg\"  # <- change this\n",
        "\n",
        "try:\n",
        "    img = load_image(example_image_path)\n",
        "    preds = classify_image_with_clip(img, text_prompts, model, processor, device=device, top_k=5)\n",
        "    show_image_with_predictions(img, preds)\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    print(\"Please update `example_image_path` to point to a real file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3551f3c6",
      "metadata": {
        "id": "3551f3c6"
      },
      "source": [
        "### What just happened?\n",
        "\n",
        "1. We loaded an **energy plant image** (or a placeholder image if you pointed to another photo).  \n",
        "2. We sent the image + a list of **text prompts** (plant components) to CLIP.  \n",
        "3. CLIP computed **similarity scores** and we converted them to probabilities with `softmax`.  \n",
        "4. We displayed the **topâ€‘k labels** (e.g., *turbine*, *valve*, *control panel*) with their probabilities.\n",
        "\n",
        "> This is already a **useful Visual AI tool**: you can drag & drop a new plant photo and instantly get a suggested label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30ca0b9",
      "metadata": {
        "id": "e30ca0b9",
        "outputId": "ec526fa1-7aea-47c2-9544-5c24cfc006cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          filename     top1_label  top1_prob\n",
              "0  switchgear1.jpg     switchgear   0.630743\n",
              "1        hand.jpeg  control panel   0.458279\n",
              "2   water tank.jpg  cooling tower   0.706054\n",
              "3         blue.jpg           pump   0.517051"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55fc641f-6dc2-4d17-8750-204ef9065726\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>top1_label</th>\n",
              "      <th>top1_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>switchgear1.jpg</td>\n",
              "      <td>switchgear</td>\n",
              "      <td>0.630743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hand.jpeg</td>\n",
              "      <td>control panel</td>\n",
              "      <td>0.458279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>water tank.jpg</td>\n",
              "      <td>cooling tower</td>\n",
              "      <td>0.706054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>blue.jpg</td>\n",
              "      <td>pump</td>\n",
              "      <td>0.517051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55fc641f-6dc2-4d17-8750-204ef9065726')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55fc641f-6dc2-4d17-8750-204ef9065726 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55fc641f-6dc2-4d17-8750-204ef9065726');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-22c8b307-4940-4548-b1c1-f892ff7fcdec\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22c8b307-4940-4548-b1c1-f892ff7fcdec')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-22c8b307-4940-4548-b1c1-f892ff7fcdec button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"Folder '{folder}' not found\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"hand.jpeg\",\n          \"blue.jpg\",\n          \"switchgear1.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top1_label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"control panel\",\n          \"pump\",\n          \"switchgear\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top1_prob\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11139641624682568,\n        \"min\": 0.45827949047088623,\n        \"max\": 0.7060539126396179,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.45827949047088623,\n          0.5170506834983826,\n          0.6307433843612671\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 8. Batch scoring for a folder of images\n",
        "\n",
        "def classify_folder(\n",
        "    folder_path: str,\n",
        "    text_prompts: List[str],\n",
        "    model: CLIPModel,\n",
        "    processor: CLIPProcessor,\n",
        "    device: str = \"cpu\",\n",
        "    top_k: int = 3,\n",
        "):\n",
        "    \"\"\"Classify all images in a folder and return a DataFrame of results.\"\"\"\n",
        "    records = []\n",
        "\n",
        "    if not os.path.isdir(folder_path):\n",
        "        raise NotADirectoryError(f\"Folder not found: {folder_path}\")\n",
        "\n",
        "    image_files = [\n",
        "        f for f in os.listdir(folder_path)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))\n",
        "    ]\n",
        "\n",
        "    for fname in image_files:\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        img = load_image(fpath)\n",
        "        preds = classify_image_with_clip(img, text_prompts, model, processor, device=device, top_k=top_k)\n",
        "\n",
        "        top1 = preds[0]\n",
        "        records.append({\n",
        "            \"filename\": fname,\n",
        "            \"top1_label\": top1[\"plain_label\"],\n",
        "            \"top1_prob\": top1[\"probability\"],\n",
        "            \"all_predictions\": preds,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example usage (update the folder name)\n",
        "folder = \"plant_images\"  # e.g. a folder with turbine/valve/etc. photos\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    batch_df = classify_folder(folder, text_prompts, model, processor, device=device, top_k=3)\n",
        "    display(batch_df[[\"filename\", \"top1_label\", \"top1_prob\"]])\n",
        "else:\n",
        "    print(f\"Folder '{folder}' not found. Create it and add images, or change the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e67236b6",
      "metadata": {
        "id": "e67236b6"
      },
      "source": [
        "## 9. Optional: Add Ground Truth Labels and Measure Accuracy\n",
        "\n",
        "If you want to turn this into a **proper miniâ€‘project**, you can:\n",
        "\n",
        "1. Create a CSV file, e.g. `labels.csv`, with columns:\n",
        "   - `filename` â€“ image file name  \n",
        "   - `true_label` â€“ the real component type (e.g., `turbine`, `valve`)  \n",
        "\n",
        "2. After running the batch classification, **join** predictions with this CSV and measure accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6cf7bd",
      "metadata": {
        "id": "6a6cf7bd"
      },
      "outputs": [],
      "source": [
        "# 9.1 Example: join predictions with ground-truth labels and compute accuracy\n",
        "\n",
        "labels_csv = \"labels.csv\"  # <-- create this file with filename,true_label\n",
        "\n",
        "if \"batch_df\" in globals() and os.path.exists(labels_csv):\n",
        "    gt = pd.read_csv(labels_csv)\n",
        "    merged = batch_df.merge(gt, on=\"filename\", how=\"inner\")\n",
        "\n",
        "    merged[\"correct\"] = merged[\"top1_label\"] == merged[\"true_label\"]\n",
        "    accuracy = merged[\"correct\"].mean()\n",
        "\n",
        "    print(f\"Number of images with labels: {len(merged)}\")\n",
        "    print(f\"Top-1 accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    display(merged[[\"filename\", \"true_label\", \"top1_label\", \"top1_prob\", \"correct\"]])\n",
        "else:\n",
        "    print(\"Either `batch_df` is not defined yet or `labels.csv` was not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b412c2c9",
      "metadata": {
        "id": "b412c2c9"
      },
      "source": [
        "## 10. Connecting Visual AI to MLOps (Optional Extension)\n",
        "\n",
        "To link this **Visual AI project** with your earlier **MLflow + Evidently** work:\n",
        "\n",
        "- Treat CLIP inference as a **model** and log:\n",
        "  - Parameters: model name, prompt set, device, etc.  \n",
        "  - Metrics: accuracy on labeled images, perâ€‘class accuracy.  \n",
        "  - Artifacts: a CSV with predictions, confusion matrix images.  \n",
        "\n",
        "- In a more advanced setup, you could:\n",
        "  - Integrate **image metadata** (component type, area, timestamp) with **sensor data** (temperature, pressure).  \n",
        "  - Build a joint dashboard: *\"show me all turbine images where pressure was above X.\"*  \n",
        "\n",
        "Below is a minimal example of logging a batch evaluation run to MLflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8f4110",
      "metadata": {
        "id": "5d8f4110"
      },
      "outputs": [],
      "source": [
        "# 10.1 Minimal MLflow logging example for the visual model (optional)\n",
        "\n",
        "import mlflow\n",
        "\n",
        "# Configure MLflow (here: local folder; change to remote server if needed)\n",
        "mlflow.set_tracking_uri(\"file:///content/mlruns\")\n",
        "mlflow.set_experiment(\"energy_plant_visual_ai_demo\")\n",
        "\n",
        "if \"batch_df\" in globals():\n",
        "    with mlflow.start_run(run_name=\"clip_visual_inference_batch\") as run:\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        mlflow.log_param(\"num_images\", len(batch_df))\n",
        "        mlflow.log_param(\"label_set\", \", \".join(plain_labels))\n",
        "\n",
        "        # If we computed accuracy with ground truth above, log it too\n",
        "        if \"accuracy\" in globals():\n",
        "            mlflow.log_metric(\"top1_accuracy\", float(accuracy))\n",
        "\n",
        "        # Save predictions as CSV\n",
        "        out_csv = \"visual_predictions.csv\"\n",
        "        batch_df.to_json(\"visual_predictions.json\", orient=\"records\", indent=2)\n",
        "        batch_df.to_csv(out_csv, index=False)\n",
        "        mlflow.log_artifact(out_csv, artifact_path=\"predictions\")\n",
        "\n",
        "        print(\"Logged visual AI batch run with MLflow. Run ID:\", run.info.run_id)\n",
        "else:\n",
        "    print(\"Run the batch classification first to create `batch_df`.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645dbd91",
      "metadata": {
        "id": "645dbd91"
      },
      "source": [
        "## 11. Project Summary and Workshop Talking Points\n",
        "\n",
        "You now have a complete **Visual AI miniâ€‘project** for an energy plant context:\n",
        "\n",
        "- **Concept:** Classify plant components (turbine, valve, control panel, etc.) from images.  \n",
        "- **Model:** Openâ€‘source **CLIP** model from Hugging Face (`openai/clip-vit-base-patch32`).  \n",
        "- **Flow:**  \n",
        "  1. Load & prepare images  \n",
        "  2. Define prompts that reflect your plant components  \n",
        "  3. Run zeroâ€‘shot inference and inspect predictions  \n",
        "  4. Batchâ€‘score folders and (optionally) evaluate against labels  \n",
        "  5. Log results to MLflow for experiment tracking  \n",
        "\n",
        "### How to present this in your workshop\n",
        "\n",
        "- Position it as: **â€œYour first Visual AI pipeline without training a CNN from scratch.â€**  \n",
        "- Emphasize:\n",
        "  - Reâ€‘using **preâ€‘trained foundation models**  \n",
        "  - The importance of **good text prompts**  \n",
        "  - How this connects with **MLOps** and monitoring over time  \n",
        "\n",
        "From here, natural next steps are:\n",
        "- Fineâ€‘tuning a vision model on your own labeled plant dataset  \n",
        "- Combining imageâ€‘based predictions with sensor data for richer risk models  \n",
        "- Deploying this as a simple web app (e.g., with Gradio or FastAPI) for technicians to upload photos from the field.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}